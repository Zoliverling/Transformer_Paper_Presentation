{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import torch\n",
    "from labml_helpers.module import Module\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from labml import experiment, lab, tracker, monit, logger\n",
    "from labml.logger import Text\n",
    "from labml.utils.download import download_file\n",
    "from labml_nn.experiments.nlp_autoregression import transpose_batch\n",
    "from labml_nn.optimizers.noam import Noam\n",
    "from labml_nn.transformers import Encoder, MultiHeadAttention\n",
    "from labml_nn.transformers.feed_forward import FeedForward\n",
    "from labml_nn.transformers.models import EmbeddingsWithPositionalEncoding, TransformerLayer\n",
    "from labml_nn.transformers.utils import subsequent_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveModel(Module):\n",
    "    \"\"\"\n",
    "    ## Auto regressive model\n",
    "    \"\"\"\n",
    "    def __init__(self, src_embed: Module, encoder: Encoder, generator: Module):\n",
    "        super().__init__()\n",
    "        self.src_embed = src_embed\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        self.src_mask = None\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            self.src_mask = subsequent_mask(len(src)).to(src.device)\n",
    "        res = self.encoder(self.src_embed(src), self.src_mask)\n",
    "        return self.generator(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Configs:\n",
    "    \"\"\"\n",
    "    ### Configurations for the Transformer model\n",
    "    This class holds all the configurations for our model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model dimensions\n",
    "    d_model: int = 512  # Dimensionality of the token embeddings\n",
    "    seq_len: int = 128  # Maximum sequence length\n",
    "    batch_size: int = 32  # Batch size for training\n",
    "\n",
    "    # Transformer specific configurations\n",
    "    n_layers: int = 6  # Number of transformer layers\n",
    "    n_heads: int = 8  # Number of attention heads\n",
    "    dropout: float = 0.1  # Dropout rate\n",
    "\n",
    "    # Feedforward network configuration\n",
    "    d_ff: int = 2048  # Dimensionality of the feedforward network\n",
    "\n",
    "    # GLU variant to use\n",
    "    glu_variant: str = 'GLU'  # Can be 'GLU', 'Bilinear', 'ReGLU', 'GEGLU', 'SwiGLU', 'ReLU', 'GELU'\n",
    "\n",
    "    # Training configurations\n",
    "    epochs: int = 5  # Number of epochs to train for\n",
    "    grad_norm_clip: float = 0.5  # Gradient clipping threshold\n",
    "\n",
    "    # Add any additional configurations you might need\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyShakespeareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ### Tiny Shakespeare Dataset\n",
    "    This is a dataset class for the Tiny Shakespeare dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_len: int):\n",
    "        \"\"\"\n",
    "        * `seq_len` is the length of the sequence of data taken for training\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Location of the text file\n",
    "        path = lab.get_data_path() / 'tiny_shakespeare.txt'\n",
    "        # Download the file if it's not present\n",
    "        if not path.exists():\n",
    "            download_file('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt', path)\n",
    "        # Read the file content\n",
    "        with open(str(path), 'r') as f:\n",
    "            text = f.read()\n",
    "        # Create a character set and map to indices\n",
    "        self.chars = sorted(set(text))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        # Encode the entire text\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long)\n",
    "        # Define sequence length\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def text_to_i(self, text: str):\n",
    "        \"\"\"\n",
    "        Transform the text into a tensor of ids\n",
    "        \"\"\"\n",
    "        return torch.tensor([self.stoi[c] for c in text], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of samples in the dataset.\n",
    "\n",
    "        *This will read the dataset `seq_len` times in a single epoch.*\n",
    "        \"\"\"\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a sample\n",
    "        \"\"\"\n",
    "        return self.data[idx:idx + self.seq_len], self.data[idx + 1:idx + self.seq_len + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa48d0fa67a48bb8f9c710989120a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\\n<span style=\"color: #C5C1Bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 182\u001b[0m\n\u001b[0;32m    178\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 182\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 178\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Start the experiment\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mstart():\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 129\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m    132\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mAutoregressiveModel.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(src):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_mask \u001b[38;5;241m=\u001b[39m subsequent_mask(\u001b[38;5;28mlen\u001b[39m(src))\u001b[38;5;241m.\u001b[39mto(src\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 15\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(res)\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\labml_nn\\transformers\\models.py:157\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Run through each transformer layer\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 157\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# Finally, normalize the vectors\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\labml_nn\\transformers\\models.py:112\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[1;34m(self, x, mask, src, src_mask)\u001b[0m\n\u001b[0;32m    110\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_self_attn(x)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Run through self attention, i.e. keys and values are from self\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m self_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Add the self attention results\u001b[39;00m\n\u001b[0;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(self_attn)\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\labml_nn\\transformers\\mha.py:197\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m    193\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Multiply by values\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mijbh,jbhd->ibhd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Save attentions for any other calculations \u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ## Trainer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs: Configs):\n",
    "        # Get the device\n",
    "        self.device = torch.device('cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda:0')\n",
    "        # Initialize the dataset\n",
    "        self.dataset = TinyShakespeareDataset(configs.seq_len)\n",
    "        # Initialize the dataloader\n",
    "        self.dataloader = DataLoader(self.dataset,\n",
    "                                     batch_size=configs.batch_size,\n",
    "                                     collate_fn=transpose_batch,\n",
    "                                     shuffle=True)\n",
    "\n",
    "        # FFN with Gated Linear Unit\n",
    "        # $$FFN_{GLU}(x)(x, W_1, V, W_2) = (\\sigma(x W_1) \\otimes x V) W_2$$\n",
    "        if configs.glu_variant == 'GLU':\n",
    "            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.Sigmoid(), True, False, False, False)\n",
    "        # FFN with Bilinear hidden layer\n",
    "        # $$FFN_{Bilinear}(x)(x, W_1, V, W_2) = (x W_1 \\otimes x V) W_2$$\n",
    "        elif configs.glu_variant == 'Bilinear':\n",
    "            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.Identity(), True, False, False, False)\n",
    "        # FFN with ReLU gate\n",
    "        # $$FFN_{ReGLU}(x)(x, W_1, V, W_2) = (\\max(0, x W_1) \\otimes x V) W_2$$\n",
    "        elif configs.glu_variant == 'ReGLU':\n",
    "            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.ReLU(), True, False, False, False)\n",
    "        # FFN with GELU gate\n",
    "        # $$FFN_{GEGLU}(x)(x, W_1, V, W_2) = (\\text{GELU}(x W_1) \\otimes x V) W_2$$\n",
    "        elif configs.glu_variant == 'GEGLU':\n",
    "            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.GELU(), True, False, False, False)\n",
    "        # FFN with Swish gate\n",
    "        # $$FFN_{SwiGLU}(x)(x, W_1, V, W_2) = (\\text{Swish}_1(x W_1) \\otimes x V) W_2$$\n",
    "        # where $\\text{Swish}_\\beta(x) = x \\sigma(\\beta x)$\n",
    "        elif configs.glu_variant == 'SwiGLU':\n",
    "            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.SiLU(), True, False, False, False)\n",
    "        # FFN with ReLU activation\n",
    "        # $$FFN_{ReLU}(x)(x, W_1, W_2, b_1, b_2) = \\text{ReLU}_1(x W_1 + b_1) W_2 + b_2$$\n",
    "        elif configs.glu_variant == 'ReLU':\n",
    "            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.ReLU())\n",
    "        # FFN with ReLU activation\n",
    "        # $$FFN_{GELU}(x)(x, W_1, W_2, b_1, b_2) = \\text{GELU}_1(x W_1 + b_1) W_2 + b_2$$\n",
    "        elif configs.glu_variant == 'GELU':\n",
    "            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.GELU())\n",
    "        else:\n",
    "            raise ValueError(f'Unknown variant {configs.glu_variant}')\n",
    "\n",
    "        # Number of different characters\n",
    "        n_chars = len(self.dataset.stoi)\n",
    "\n",
    "        # Initialize [Multi-Head Attention module](../mha.html)\n",
    "        mha = MultiHeadAttention(configs.n_heads, configs.d_model, configs.dropout)\n",
    "        # Initialize the [Transformer Block](../models.html#TransformerLayer)\n",
    "        transformer_layer = TransformerLayer(d_model=configs.d_model, self_attn=mha, src_attn=None,\n",
    "                                             feed_forward=ffn, dropout_prob=configs.dropout)\n",
    "        # Initialize the model with an\n",
    "        # [embedding layer](../models.html#EmbeddingsWithPositionalEncoding)\n",
    "        # (with fixed positional encoding)\n",
    "        # [transformer encoder](../models.html#Encoder) and\n",
    "        # a linear layer to generate logits.\n",
    "        self.model = AutoregressiveModel(EmbeddingsWithPositionalEncoding(configs.d_model, n_chars),\n",
    "                                         Encoder(transformer_layer, configs.n_layers),\n",
    "                                         nn.Linear(configs.d_model, n_chars))\n",
    "\n",
    "        # Move the model to the current device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Initialize [Noam optimizer](../../optimizers/noam.html)\n",
    "        self.optimizer = Noam(self.model.parameters(), lr=1.0, warmup=2_000, d_model=configs.d_model)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        # Number of training epochs;\n",
    "        # *note that our dataset definition repeats the data `seq_len` times in a single epoch*\n",
    "        self.epochs = configs.epochs\n",
    "        # Gradient clipping norm\n",
    "        self.grad_norm_clip = configs.grad_norm_clip\n",
    "\n",
    "        # Set tracker configurations\n",
    "        tracker.set_scalar(\"loss.*\", True)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        ### Sampling function to generate samples periodically while training\n",
    "        \"\"\"\n",
    "\n",
    "        # Starting prompt\n",
    "        prompt = 'It is'\n",
    "        # Collect output for printing\n",
    "        log = [(prompt, Text.subtle)]\n",
    "        # Sample 25 tokens\n",
    "        for i in monit.iterate('Sample', 25):\n",
    "            # Tokenize the prompt\n",
    "            data = self.dataset.text_to_i(prompt).unsqueeze(-1)\n",
    "            data = data.to(self.device)\n",
    "            # Get the model output\n",
    "            output = self.model(data)\n",
    "            # Get the model prediction (greedy)\n",
    "            output = output.argmax(dim=-1).squeeze()\n",
    "            # Add the prediction to prompt\n",
    "            prompt += self.dataset.itos[output[-1].item()]\n",
    "            # Add the prediction for logging\n",
    "            log += [(self.dataset.itos[output[-1].item()], Text.value)]\n",
    "\n",
    "        # Print the sampled output\n",
    "        logger.log(log)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        ### Train the model\n",
    "        \"\"\"\n",
    "\n",
    "        # Loop for the given number of epochs\n",
    "        for _ in monit.loop(self.epochs):\n",
    "            # Iterate over the minibatches\n",
    "            for i, batch in monit.enum('Train', self.dataloader):\n",
    "                # Move data to the device\n",
    "                data, target = batch[0].to(self.device), batch[1].to(self.device)\n",
    "\n",
    "                # Set tracker step, as the number of characters trained on\n",
    "                tracker.add_global_step(data.shape[0] * data.shape[1])\n",
    "\n",
    "                # Set model state to training\n",
    "                self.model.train()\n",
    "                # Evaluate the model\n",
    "                output = self.model(data)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.loss_func(output.view(-1, output.shape[-1]), target.view(-1))\n",
    "                # Log the loss\n",
    "                tracker.add(\"loss.train\", loss)\n",
    "\n",
    "                # Calculate gradients\n",
    "                loss.backward()\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n",
    "                # Take optimizer step\n",
    "                self.optimizer.step()\n",
    "                # Log the model parameters and gradients\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    tracker.add('model', self.model)\n",
    "                # Clear the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Generate a sample\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    self.model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        self.sample()\n",
    "\n",
    "                # Save the tracked metrics\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    tracker.save()\n",
    "\n",
    "            # Save the model\n",
    "            experiment.save_checkpoint()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create experiment\n",
    "    experiment.create(name=\"glu_variants\")\n",
    "    # Create configs\n",
    "    configs = Configs()\n",
    "    # Load configurations\n",
    "    experiment.configs(dataclasses.asdict(configs))\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(configs)\n",
    "    # Set models for training and loading\n",
    "    experiment.add_pytorch_models({'model': trainer.model})\n",
    "\n",
    "    # Start the experiment\n",
    "    with experiment.start():\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
