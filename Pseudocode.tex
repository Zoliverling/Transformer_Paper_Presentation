\documentclass{article}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath}

\title{Transformer with GLU Variant Pseudocode}
\author{Your Name}

\begin{document}

\maketitle

\begin{algorithm}
\caption{Transformer model with Gated Linear Unit (GLU) variant}
\begin{algorithmic}[1]

\Require $x$, a sequence of token IDs.
\Ensure $y$, the output sequence after passing through the Transformer model with a GLU variant.
\State Hyperparameters: $N$, the number of layers; $d_{\text{model}}$, the dimensionality of token embeddings; $d_{\text{ff}}$, the dimensionality of the feedforward layer; $h$, the number of attention heads.

\State Parameters $\theta$ include all following parameters:
\State \hspace{\algorithmicindent} $E \in \mathbb{R}^{d_{\text{vocab}} \times d_{\text{model}}}$, the token embedding matrix.
\State \hspace{\algorithmicindent} $PE \in \mathbb{R}^{\text{max\_position} \times d_{\text{model}}}$, the positional embedding matrix.
\State \hspace{\algorithmicindent} For each layer $l \in [1...N]$:
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $W_Q^l, W_K^l, W_V^l \in \mathbb{R}^{d_{\text{model}} \times (d_{\text{model}}/h)}$, attention parameter matrices for each head.
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $W_O^l \in \mathbb{R}^{(d_{\text{model}}/h) \times d_{\text{model}}}$, output projection matrix for multi-head attention.
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\gamma^l, \beta^l \in \mathbb{R}^{d_{\text{model}}}$, parameters for layer normalization before and after the multi-head attention, respectively.
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $W_1^l \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}$, $W_2^l \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}$, weights for the feedforward network.
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $W_g^l \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}$, weights for the gating mechanism in the GLU variant.
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $b_1^l, b_2^l, b_g^l \in \mathbb{R}^{d_{\text{model}}}$, biases for the feedforward network and the gating mechanism.
\State \hspace{\algorithmicindent} $W_u \in \mathbb{R}^{d_{\text{model}} \times d_{\text{vocab}}}$, the unembedding matrix.

\State Initialize the output sequence $y$ to an empty list.
\State Compute the embedded input sequence $E_x = E[x] + PE[\text{pos}]$, where $\text{pos}$ is the position sequence.
\For{each layer $l \in [1...N]$}
    \State Apply layer normalization: $X_{\text{norm}} = \text{LayerNorm}(E_x, \gamma^l, \beta^l)$.
    \State Calculate self-attention: $Z = \text{MultiHeadAttention}(X_{\text{norm}}, W_Q^l, W_K^l, W_V^l, W_O^l)$.
    \State Apply residual connection and layer normalization: $X_{\text{norm}} = \text{LayerNorm}(X_{\text{norm}} + Z, \gamma^l, \beta^l)$.
    \State Apply the first feedforward projection: $F1 = W_1^lX_{\text{norm}} + b_1^l$.
    \State Apply the GLU variant: $G = \text{GLU\_Variant}(F1, W_g^l, b_g^l)$.
    \State Apply the second feedforward projection: $F2 = W_2^lG + b_2^l$.
    \State Apply residual connection: $E_x = X_{\text{norm}} + F2$.
    \State Append the result to the output sequence $y$.
\EndFor
\State Compute the unembedded output: $P = \text{softmax}(W_uy)$.

\State \textbf{return} $P$

\Function{GLU\_Variant}{$X, W_g, b_g$}
    \State Split the input matrix into two equal parts: $A, B = \text{split}(X, 2, \text{axis}=-1)$.
    \State Apply a non-linearity to the first part: $A = \tanh(A)$.
    \State Apply the gating mechanism to the second part: $B = \text{sigmoid}(W_gB + b_g)$.
    \State Element-wise multiplication of the results: $G = A \times B$.

    \State \textbf{return} $G$
\EndFunction

\end{algorithmic}
\end{algorithm}

\end{document
