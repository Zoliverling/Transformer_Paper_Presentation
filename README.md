# GLU Variants

## Introdcution
Transformers have become a dominant architecture in the field of natural language processing. However, their capacity for modeling complex dependencies can be limited by the standard feed-forward networks used within. The introduction of GLU variants into the feed-forward layers offers a novel approach to enhancing the model's representational power.

## Structure
The main idea is to replace the standard feed-forward networks in Transformer layers with a variant of Gated Linear Units (GLUs). These units introduce a gating mechanism over the linear transformation, which allows the model to control the flow of information more effectively.

